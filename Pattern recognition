---
title: "Распознавание образов"
---

Необходимо построить правило классификации кварталов.

#kNN

Сначала произведем нормализацию признаков, т.к. наш метод зависит от расстояний между объектов.

```{r, echo = F}
options(warn = -1)
library(MASS)
library(class)
library(knitr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
df = Boston
```

```{r}
scaled.df = scale(df)
```

Смоделируем классификацию. Кварталы классифицируются на две группы:  "богатые" (для которых значение переменной "цена" выше медианы), и "бедные" (для которых значение переменной "цена" ниже медианы).

```{r}
cl = rep(0, nrow(scaled.df))
m = median(scaled.df[,14])

for (i in 1:nrow(scaled.df)){
  if (scaled.df[i, 14] >= m){
    cl[i] = 1
    }
}
```

Разделим выборку на $test$ и $train$.

```{r}
set.seed(1234)
test.num = sample(1:nrow(scaled.df), 169, replace = FALSE)

test = scaled.df[test.num, -c(1:4, 9, 14)]
train = scaled.df[-test.num, -c(1:4, 9, 14)]
```

Подберем значение $k$ перебором.

```{r}
a = matrix(rep(0,15), nrow = 1)
colnames(a) = c(1:15)
rownames(a) = c("sum. error")

for (i in 1:15){
  zzz = knn(train, test, cl[-test.num], k = i)
  a[i] = sum(zzz != cl[test.num])
}
kable(a, align = "l")
```

Вполне целесообразно остановиться на значении $k = 3$.
Построим прогноз и таблицу сопряженности.

```{r}
zzz = knn(train, test, cl[-test.num], k = 3)
table(zzz, cl[test.num])
```

####Вывод
В процентном соотношении мы имеем $13\%$ ошибок. Поэтому мы можем говорить, что модель вполне рабочая.

#Classification tree

Разделим выборку на $test$ и $train$.

```{r, echo = F}
df = data.frame(df, cl)
df$rad = as.factor(df$rad)
```

```{r, echo = T}
test = df[test.num, -c(1:4, 9, 14)]
train = df[-test.num, -c(1:4, 9, 14)]
```

Попробуем подобрать параметры нашей модели. В качестве параметра качества будем использовать $\%$ ошибок.

```{r, echo = F}
ans = matrix(rep(0, 10), nrow = 2)
rownames(ans) = c("train", "test")
colnames(ans) = c("(5; 2; 6)", "(10; 5; 4)", "(10; 5; 3)", "(10; 5; 2)", "(20; 10; 4)")
i = c(5, 10, 10, 10, 20)
j = c(2, 5, 5, 5, 10)
k = c(6, 4, 3, 2, 4)

for (ind in 1:5){
  res = rpart(cl ~ ., data = train, method = "class",
            control = rpart.control(minsplit = i[ind], minbucket = j[ind], maxdepth = k[ind]))
  
  ans[1, ind] = round(sum(train$cl != predict(res, train[ , -9], type = "class"))/nrow(train), 2)
  ans[2, ind] = round(sum(test$cl != predict(res, test[ , -9], type = "class"))/nrow(test), 2)
}

kable(ans, align = "l")
```

Следуя правилу KISS, остановимся на параметрах (10; 5; 2)

```{r}
res = rpart(cl ~ ., data = train, method = "class",
            control = rpart.control(minsplit = 10, minbucket = 5, maxdepth = 2))

rpart.plot(res, type = 2, extra = 1)


table(train$cl, predict(res, train[ , -9], type = "class"))
table(test$cl, predict(res, test[ , -9], type = "class"))
```

####Вывод
В процентном соотношении мы имеем $14\%$ ошибок. Данная модель очень проста и это её $+$: мы используем всего 2 параметра (2 неравенства).

#Random Forest

Снова выделим $train$ и $test$.
```{r}
df = Boston
test = df[test.num, -c(1:4, 9, 14)]
train = df[-test.num, -c(1:4, 9, 14)]

cl.1 = as.factor(cl)
```

Запустим процедуру $randomForest$ с $ntree = 200$ и попробуем найти оптимальное значение параметра $ntree$.

```{r}
set.seed(1234)
res = randomForest(train, y = cl.1[-test.num], ntree = 200, mtry = floor(sqrt(ncol(train))),
                       replace = FALSE, nodesize = 1,
                       importance = TRUE, localImp = FALSE,
                       proximity = FALSE, norm.votes = TRUE, do.trace = 10,
                       keep.forest = T, corr.bias = FALSE, keep.inbag = FALSE)
```

По моему мнению, оптимальным значением количества деревьев явлется $60$.

```{r}
set.seed(1234)
res = randomForest(train, y = cl.1[-test.num], ntree = 60, mtry = floor(sqrt(ncol(train))),
                   replace = FALSE, nodesize = 1,
                   importance = TRUE, localImp = FALSE,
                   proximity = FALSE, norm.votes = TRUE, do.trace = 70,
                   keep.forest = T, corr.bias = FALSE, keep.inbag = FALSE)

table(cl.1[-test.num], predict(res, train))
round(sum(cl[-test.num] != predict(res, train))/nrow(train), 2)
table(cl.1[test.num], predict(res, test))

round(sum(cl[test.num] != predict(res, test))/nrow(test), 2)
```

Как мы видим, на $train$ выборке наш лес не ошибается, но на тестовой выборке мы имеем $11\%$ ошибок. Можно подумать, что здесь имеет место переобучение. Попробуем уменьшить число деревьев и увеличить $nodesize$.

```{r}
set.seed(1234)
res = randomForest(train, y = cl.1[-test.num], ntree = 20, mtry = floor(sqrt(ncol(train))),
                   replace = FALSE, nodesize = 10,
                   importance = TRUE, localImp = FALSE,
                   proximity = FALSE, norm.votes = TRUE, do.trace = 41,
                   keep.forest = T, corr.bias = FALSE, keep.inbag = FALSE)

table(cl.1[-test.num], predict(res, train))
round(sum(cl[-test.num] != predict(res, train))/nrow(train), 2)

table(cl.1[test.num], predict(res, test))
round(sum(cl.1[test.num] != predict(res, test))/nrow(test), 2)
```

Методом перебора мы добились $9\%$ ошибок, что неплохо.

####Информативность переменных.

```{r}
varImpPlot(res, sort = F)
```

#GBM

```{r, echo = F}
df = Boston

cl = rep(0, nrow(df))
m = median(df[,14])

for (i in 1:nrow(df)){
  if (df[i, 14] >= m){
    cl[i] = 1
  }
}
df = data.frame(df, cl)

set.seed(1234)
test.num = sample(1:nrow(df), 169, replace = FALSE)


test = df[test.num, -c(1:4, 9, 14)]
train = df[-test.num, -c(1:4, 9, 14)]
```


```{r}
set.seed(1234)
gbm.res = gbm(cl ~. , data = train, distribution = "gaussian", n.trees = 200,
               shrinkage = 0.05, interaction.depth = 3, bag.fraction = 0.66,
               n.minobsinnode = 10, cv.folds = 0, keep.data = T, verbose = T)
```

Разумно остановиться на $n.trees = 80$. Построим на результаты.

* #### train ####

```{r}
pr = predict(gbm.res, newdata = train[, -c(9)], n.trees = 80)
pr.2 = rep(0, nrow(train))
pr.2[(pr > 0.6)] = 1
table(cl[-test.num], pr.2)
round(sum(cl[-test.num] != pr.2)/nrow(train), 2)
```

* #### test ####

```{r}
pr = predict(gbm.res, newdata = test[, -c(9)], n.trees = 80)
pr.2 = rep(0, nrow(test))
pr.2[(pr > 0.6)] = 1
table(cl[test.num], pr.2)
round(sum(cl[test.num] != pr.2)/nrow(test), 2)
```
